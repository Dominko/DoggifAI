{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272535\n",
      "79545\n",
      "81881\n",
      "433961\n",
      "433915\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Bio import SeqIO\n",
    "\n",
    "heavy_path = \"../../datasets/raw/Heavy_variable_dog_unique_functional.fasta\"\n",
    "kappa_path = \"../../datasets/raw/Light_variable_kappa_dog_unique_functional.fasta\"\n",
    "lambda_path = \"../../datasets/raw/Light_variable_lambda_dog_unique_functional.fasta\"\n",
    "\n",
    "i = 0\n",
    "records = []\n",
    "seqs = []\n",
    "with open(heavy_path) as handle:\n",
    "    j = 0\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        records.append(record)\n",
    "        seqs.append(str(record.seq))\n",
    "        # break\n",
    "        i+=1\n",
    "        j+=1\n",
    "    print(j)\n",
    "with open(kappa_path) as handle:\n",
    "    j = 0\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        records.append(record)\n",
    "        seqs.append(str(record.seq))\n",
    "        # break\n",
    "        i+=1\n",
    "        j+=1\n",
    "    print(j)\n",
    "with open(lambda_path) as handle:\n",
    "    j = 0\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        records.append(record)\n",
    "        seqs.append(str(record.seq))\n",
    "        # break\n",
    "        i+=1\n",
    "        j+=1\n",
    "    print(j)\n",
    "print(len(seqs))\n",
    "print(len(set(seqs)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if CDR - FR pairs are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425283\n",
      "255800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read all ft_input files\n",
    "train_ft_input_path = '../../datasets/doggy_data/train_ft_input'\n",
    "val_ft_input_path = '../../datasets/doggy_data/val_ft_input'\n",
    "test_ft_input_path = '../../datasets/doggy_data/test_ft_input'\n",
    "\n",
    "train_ft_output_path = '../../datasets/doggy_data/train_ft_output'\n",
    "val_ft_output_path = '../../datasets/doggy_data/val_ft_output'\n",
    "test_ft_output_path = '../../datasets/doggy_data/test_ft_output'\n",
    "\n",
    "seqs = []\n",
    "\n",
    "with open(train_ft_input_path, \"r\") as handle:\n",
    "    for seq in handle.readlines():\n",
    "        seqs.append(seq.replace('\\r\\n', \"\"))\n",
    "with open(val_ft_input_path, \"r\") as handle:\n",
    "    for seq in handle.readlines():\n",
    "        seqs.append(seq.replace('\\r\\n', \"\"))\n",
    "with open(val_ft_input_path, \"r\") as handle:\n",
    "    for seq in handle.readlines():\n",
    "        seqs.append(seq.replace('\\r\\n', \"\"))\n",
    "print(len(seqs))\n",
    "\n",
    "print(len(set(seqs)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "EVQLVESGGDQVKPAGSLRLSCVASGFTFSSYDMSWVRQAPGKGLQWVAGINSGGSITGYADAVKGRFTISRDNARNTVYLQMNSLRAEDTAMYYCAKPPRYSDFWGQGTLVTVSS\n",
      "EVQLVESGGDQVKPAGSLRLSCVASGFTFSSYDMSWVRQAPGKGLQWVAGINSGGSITGYADAVKGRFTISRDNARNTVYLQMNSLRAEDTAMYYCAKPPRYSDFWGQGTLVTVSS\n",
      "1.8193111187954436\n",
      "-0.25689655172413794\n",
      "35.170689655172396\n",
      "12543.89649999999\n",
      "870.0\n"
     ]
    }
   ],
   "source": [
    "from Bio import Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "train_pt_path = '../../datasets/doggy_data/train_pt'\n",
    "\n",
    "seqs = []\n",
    "\n",
    "with open(train_pt_path, \"r\") as handle:\n",
    "    for seq in handle.readlines():\n",
    "        seqs.append(seq.replace('\\n', \"\").replace(\"<heavy>\", \"\").replace(\"<light_kappa>\", \"\").replace(\"<light_lambda>\", \"\"))\n",
    "\n",
    "# i_1 = random.randint(0, len(seqs))\n",
    "# i_2 = random.randint(0, len(seqs))\n",
    "\n",
    "i_1 = 1\n",
    "i_2 = 1\n",
    "\n",
    "print(i_1)\n",
    "print(i_2)\n",
    "\n",
    "print(seqs[i_1])\n",
    "print(seqs[i_2])\n",
    "\n",
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = -11\n",
    "aligner.extend_gap_score = -1\n",
    "aligner.substitution_matrix = substitution_matrices.load(name='PAM30')\n",
    "alignment = aligner.align(seqs[i_1], seqs[i_2])\n",
    "\n",
    "print(ProtParam.ProteinAnalysis(seqs[i_1]).charge_at_pH(7))\n",
    "print(ProtParam.ProteinAnalysis(seqs[i_1]).gravy())\n",
    "print(ProtParam.ProteinAnalysis(seqs[i_1]).instability_index())\n",
    "print(ProtParam.ProteinAnalysis(seqs[i_1]).molecular_weight())\n",
    "\n",
    "print(alignment[0].score)\n",
    "\n",
    "# print(format_alignment(*alignment[0], full_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42528/42528 [00:07<00:00, 5367.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866.4464588036118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Bio import Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "val_pt_path = '../../datasets/doggy_data/val_pt'\n",
    "\n",
    "seqs = []\n",
    "\n",
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = -11\n",
    "aligner.extend_gap_score = -1\n",
    "aligner.substitution_matrix = substitution_matrices.load(name='PAM30')\n",
    "\n",
    "i = 0\n",
    "sum_score = 0\n",
    "\n",
    "with open(val_pt_path, \"r\") as handle:\n",
    "    handle.readline()\n",
    "    for seq in tqdm(handle.readlines()):\n",
    "        seq = seq.replace('\\n', \"\").replace(\"<heavy>\", \"\").replace(\"<light_kappa>\", \"\").replace(\"<light_lambda>\", \"\")\n",
    "        alignment = aligner.align(seq, seq)\n",
    "        i += 1\n",
    "        sum_score += alignment[0].score\n",
    "\n",
    "print(sum_score/i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML parsing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <heavy>EGQLAESGGDLVKPAGSLRLSCVASGLAFN<extra_id_0> QAPEKGLQLVAG<extra_id_1> AGSRT<extra_id_2> YADAVKGRFTISRDS<extra_id_3> TVFLQMNSLTAEDTAVYYCAKVATDFRGPGAAYGLEYWGQ<extra_id_4> LVTVS<extra_id_5><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "<extra_id_1><extra_id_2> G<extra_id_3> A<extra_id_4> GGGGTA<extra_id_5> TVTVTVSSS</s></s></s></s></s></s></s></s></s></s> S</s> SSSSSS</s> GTVSSSS</s> TVSSS</s></s></s> S</s><extra_id_5> SSSSSS</s> S</s> TVS</s> TVSSSS</s></s></s></s> S</s> GQGQL</s> GTVSSS</s></s></s></s></s></s></s></s></s> S</s> SSSS</s> S</s> GGTVSSSS</s></s></s></s> S</s></s></s></s></s></s></s></s></s> G<extra_id_1> GGGG<extra_id_4>GTLQWG\n",
      "target            0 EGQLAESGGDLVKPAGSLRLSCVASGLAFNKYSMNWVRQAPEKGLQLVAGIGNAGSRTYY\n",
      "                  0 ||||||||||||||||||||||||||||||--------||||||||||||---|||||.|\n",
      "query             0 EGQLAESGGDLVKPAGSLRLSCVASGLAFN--------QAPEKGLQLVAG---AGSRTGY\n",
      "\n",
      "target           60 ADAVKGRFTISRDSAVNTVFLQMNSLTAEDTAVYYCAKVATDFRGPGAAYGLEYWGQG--\n",
      "                 60 |||||||||||||||--|||||||||||||||||||||||||||||||||||||||||--\n",
      "query            49 ADAVKGRFTISRDSA--TVFLQMNSLTAEDTAVYYCAKVATDFRGPGAAYGLEYWGQGGG\n",
      "\n",
      "target          118 --TLVTVS 124\n",
      "                120 --.||||| 128\n",
      "query           107 GTALVTVS 115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Bio import Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "import re\n",
    "\n",
    "shit_output = \"<extra_id_1><extra_id_2> G<extra_id_3> A<extra_id_4> GGGGTA<extra_id_5> TVTVTVSSS</s></s></s></s></s></s></s></s></s></s> S</s> SSSSSS</s> GTVSSSS</s> TVSSS</s></s></s> S</s><extra_id_5> SSSSSS</s> S</s> TVS</s> TVSSSS</s></s></s></s> S</s> GQGQL</s> GTVSSS</s></s></s></s></s></s></s></s></s> S</s> SSSS</s> S</s> GGTVSSSS</s></s></s></s> S</s></s></s></s></s></s></s></s></s> G<extra_id_1> GGGG<extra_id_4>GTLQWG\"\n",
    "output = \"<extra_id_0> KYSMNWVR<extra_id_1> IGN<extra_id_2> Y<extra_id_3> AVN<extra_id_4> GT<extra_id_5> S</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\"\n",
    "input = \"<s> <heavy>EGQLAESGGDLVKPAGSLRLSCVASGLAFN<extra_id_0> QAPEKGLQLVAG<extra_id_1> AGSRT<extra_id_2> YADAVKGRFTISRDS<extra_id_3> TVFLQMNSLTAEDTAVYYCAKVATDFRGPGAAYGLEYWGQ<extra_id_4> LVTVS<extra_id_5><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\"\n",
    "\n",
    "extra_id_template = \"<extra_id_?>\"\n",
    "extra_id_len = len(extra_id_template)\n",
    "\n",
    "def strip_tags(sequence):\n",
    "    sequence = re.sub(\"<[^>]+>\", \"\", sequence)\n",
    "    return sequence.replace(\" \", \"\")\n",
    "\n",
    "def reconstruct_pt_sequence(sequence, generated ):\n",
    "    sequence = sequence.replace(\" \", \"\")\n",
    "    generated = generated.replace(\" \", \"\")\n",
    "    i = 0\n",
    "    reconstructed = sequence\n",
    "    while True:\n",
    "        # Find next id in input\n",
    "        extra_id = extra_id_template.replace(\"?\", str(i))\n",
    "        \n",
    "        idx = sequence.find(extra_id)\n",
    "\n",
    "        # Check if extra id exsits\n",
    "        if idx == -1:\n",
    "            return strip_tags(reconstructed)\n",
    "\n",
    "        # Find extra id in generated\n",
    "        start_idx = generated.find(extra_id)\n",
    "        # If extra id is not found, skip (i.e. replace with \"\")\n",
    "        if start_idx == -1:\n",
    "            i += 1\n",
    "            continue\n",
    "        generated = generated[start_idx+extra_id_len:]\n",
    "        end_idx = generated.find(\"<\")\n",
    "        if start_idx == -1:\n",
    "            end_idx = len(generated)-1\n",
    "\n",
    "        chunk = generated[:end_idx]\n",
    "        reconstructed = reconstructed.replace(extra_id, chunk)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "print(input)\n",
    "print(shit_output)\n",
    "original = reconstruct_pt_sequence(input, output)\n",
    "generated = reconstruct_pt_sequence(input, shit_output)\n",
    "\n",
    "\n",
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = -11\n",
    "aligner.extend_gap_score = -1\n",
    "aligner.substitution_matrix = substitution_matrices.load(name='PAM30')\n",
    "\n",
    "alignment = aligner.align(original, generated)\n",
    "print(alignment[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GVQLAESGGDLVKPGGSLRLSCVASGLILSSIGMTWVRQSPGKELQWVADLASSGNTYYTDAVKGRFTISRDNAKNTLYLQMDSLRAEDTAVYFCTTGRRGYWGQGTLVTVSS\n",
      "  ||||||||||||||||||||||||||||||||.||||||||.|||||||||||||||.||||||||||||||||||||||.||||||||||.|||||||||||||||||||\n",
      "EGQLAESGGDLVKPGGSLRLSCVASGLILSSIGMSWVRQSPGKGLQWVADLASSGNTYYADAVKGRFTISRDNAKNTLYLQMNSLRAEDTAVYYCTTGRRGYWGQGTLVTVSS\n",
      "\n",
      "783.0\n"
     ]
    }
   ],
   "source": [
    "from Bio import Align\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SeqUtils import ProtParam\n",
    "from Bio.Align import substitution_matrices\n",
    "import re\n",
    "\n",
    "# original = \"EVQLVESGGDLVKPGGSLRLSCVASGFSFSSCNMGWVRQAPGKGLQWVAWIKTRGSDTSYADAVKGRFTISRDDAKNTLYLQMNSLREEDTAMYYCVRAVNGWYEVHNYDYWGQGTLVTVSS\"\n",
    "# new = \"EGQLAESGGDLVKPGGSLRLSCVASGFSFSSCNMSWVRQSPGKGLQWVADIKTRGSDTYYADAVKGRFTISRDNAKNTLYLQMNSLRAEDTAVYYCVRAVNGWYEVHNYDYWGQGTLVTVSS\" \n",
    "\n",
    "original = \"GVQLAESGGDLVKPGGSLRLSCVASGLILSSIGMTWVRQSPGKELQWVADLASSGNTYYTDAVKGRFTISRDNAKNTLYLQMDSLRAEDTAVYFCTTGRRGYWGQGTLVTVSS\"\n",
    "new = \"EGQLAESGGDLVKPGGSLRLSCVASGLILSSIGMSWVRQSPGKGLQWVADLASSGNTYYADAVKGRFTISRDNAKNTLYLQMNSLRAEDTAVYYCTTGRRGYWGQGTLVTVSS\" \n",
    "\n",
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = -11\n",
    "aligner.extend_gap_score = -1\n",
    "aligner.substitution_matrix = substitution_matrices.load(name='PAM30')\n",
    "\n",
    "alignment = aligner.align(original, new)\n",
    "print(alignment[0])\n",
    "print(alignment.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.3.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__(self, ...) called with invalid or missing `self` argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecoder\u001b[39;00m \u001b[39mimport\u001b[39;00m CTCDecoderLM, CTCDecoderLMState\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m CTCDecoderLM\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__(self, ...) called with invalid or missing `self` argument"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "from torchaudio.models.decoder import CTCDecoderLM, CTCDecoderLMState\n",
    "import torch\n",
    "\n",
    "CTCDecoderLM.__init__(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "data_dir = '../../datasets/doggy_data/train_pt'\n",
    "pattern = r'<[^>]+>'\n",
    "\n",
    "with open(data_dir, \"r\") as handle:\n",
    "    line = handle.readline()\n",
    "    line = handle.readline()\n",
    "    line = re.sub(pattern, '', line).strip()\n",
    "    i = 0\n",
    "    longest = len(line)\n",
    "    while line:\n",
    "        line = handle.readline()\n",
    "        line = re.sub(pattern, '', line).strip()\n",
    "        if len(line) < longest and len(line) != 0 :\n",
    "            longest = len(line)\n",
    "\n",
    "print(i)\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340226\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "data_dir = '../../datasets/doggy_data/train_pt'\n",
    "pattern = r'<[^>]+>'\n",
    "\n",
    "with open(data_dir, \"r\") as handle:\n",
    "    line = handle.readline()\n",
    "    \n",
    "    i = 1\n",
    "    while line:\n",
    "        line = handle.readline()\n",
    "        i+=1\n",
    "\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike_rna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01f607ab95c8902d71e46e818ba88ea9631de7ba0f060bf546ce1a8687d18deb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

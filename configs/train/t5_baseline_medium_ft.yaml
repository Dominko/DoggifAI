dataset_configs:
  train:
    input_sequences_path: ./datasets/doggy_data/train_ft_input
    output_sequences_path: ./datasets/doggy_data/train_ft_output
  val:
    input_sequences_path: ./datasets/doggy_data/val_ft_input
    output_sequences_path: ./datasets/doggy_data/val_ft_output
  test:
    input_sequences_path: ./datasets/doggy_data/test_ft_input
    output_sequences_path: ./datasets/doggy_data/test_ft_output
  corrupted_percentage: 0.15
  mean_noise_span_length: 3.0
  extra_ids: 100
model_configs:
  model_type: t5_simple
  model_state_dict_path: ./train_outputs/2024_07_09__18_35_18/checkpoint_50.pt
  tokenizer: Base
  tokenizer_path: ./datasets/vocab/test.model
  hyperparameters:
    max_seq_len: 150
    hidden_dim: 256
    nhead: 4
    num_layers: 4
    ff_size: 2048
    regularizers: []
    dropout: 0.1
    batch_size: 256
    optimizer: "adam"
    learning_rate: 1e-4
    grad_accumulation_step: 1
    immunogenicity_as_value: False
training_configs:
  epochs: 100
  eval_steps: 1
  checkpoint_steps: 1
  device: 0
  random_seed: 1234
  outputs_dir: "ft_train_outputs"
  training_type: "ft"
  wandb_project: "doggy_ai_ft"
  wandb_name: "doggy_ai_ft_medium"
  verbose: True
  verbose_output_file: "debug_outputs"
  samples_output_file: "samples_output"
validation_configs:
  substitution_matrix: "PAM30"
  gap_insertion_penalty: -11
  gap_extension_penalty: -1
  sample_method: "topk"
  beam_width: 1
  top_k: 1

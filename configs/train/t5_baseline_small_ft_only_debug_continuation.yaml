dataset_configs:
  train:
    input_sequences_path: ./datasets/doggy_data/train_ft_input_100
    output_sequences_path: ./datasets/doggy_data/train_ft_output_100
  val:
    input_sequences_path: ./datasets/doggy_data/val_ft_input_100
    output_sequences_path: ./datasets/doggy_data/val_ft_output_100
  test:
    input_sequences_path: ./datasets/doggy_data/train_ft_input_100
    output_sequences_path: ./datasets/doggy_data/train_ft_output_100
  corrupted_percentage: 0.15
  mean_noise_span_length: 3.0
  extra_ids: 100
model_configs:
  model_type: t5_simple
  tokenizer: Base
  tokenizer_path: ./datasets/vocab/test.model
  hyperparameters:
    max_seq_len: 150
    hidden_dim: 256
    nhead: 4
    num_layers: 4
    ff_size: 2048
    regularizers: []
    dropout: 0.1
    batch_size: 32
    optimizer: "adam"
    learning_rate: 1e-4
    grad_accumulation_step: 1
    immunogenicity_as_value: False
  model_state_dict_path: ./ft_train_outputs/2024_08_03__04_28_37/checkpoint_15.pt
  model_id: ygvykiai
  start_epoch: 90
training_configs:
  epochs: 500
  eval_steps: 1
  checkpoint_steps: 600
  device: 0
  random_seed: 1234
  outputs_dir: "ft_train_outputs"
  training_type: "ft"
  wandb_project: "doggy_ai_ft"
  wandb_name: "doggy_ai_ft_small_no_pt"
  verbose: True
  verbose_output_file: "debug_outputs"
  samples_output_file: "samples_output"
validation_configs:
  substitution_matrix: "PAM30"
  gap_insertion_penalty: -11
  gap_extension_penalty: -1
  sample_method: "topk"
  beam_width: 50
  top_k: 1

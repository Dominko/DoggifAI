dataset_configs:
  train:
    output_sequences_path: ./datasets/doggy_data/train_pt
  val:
    output_sequences_path: ./datasets/doggy_data/val_pt
  test:
    output_sequences_path: ./datasets/doggy_data/test_pt
  corrupted_percentage: 0.15
  mean_noise_span_length: 3.0
  extra_ids: 100
model_configs:
  model_type: t5_simple
  tokenizer: Base
  tokenizer_path: ./datasets/vocab/test.model
  hyperparameters:
    max_seq_len: 150
    hidden_dim: 1024
    nhead: 16
    num_layers: 8
    ff_size: 4096
    regularizers: []
    dropout: 0.1
    batch_size: 256
    optimizer: "adam"
    learning_rate: 1e-4
    grad_accumulation_step: 1
    immunogenicity_as_value: False
training_configs:
  epochs: 100
  eval_steps: 1
  checkpoint_steps: 10
  device: 0
  random_seed: 1234
  outputs_dir: "train_outputs"
  training_type: "pt"
  wandb_project: "doggy_ai_pt"
  wandb_name: "doggy_ai_pt_large"
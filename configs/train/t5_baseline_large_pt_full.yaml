dataset_configs:
  train:
    output_sequences_path: ./datasets/doggy_data/train_pt_10mil
  val:
    output_sequences_path: ./datasets/doggy_data/val_pt_10mil
  test:
    output_sequences_path: ./datasets/doggy_data/test_pt_10mil
  corrupted_percentage: 0.15
  mean_noise_span_length: 3.0
  extra_ids: 100
model_configs:
  model_type: t5_simple
  tokenizer: Base
  tokenizer_path: ./datasets/vocab/test.model
  hyperparameters:
    max_seq_len: 188
    hidden_dim: 1024
    nhead: 16
    num_layers: 8
    ff_size: 4096
    regularizers: []
    dropout: 0.1
    batch_size: 128
    optimizer: "adam"
    learning_rate: 1e-5
    grad_accumulation_step: 1
    immunogenicity_as_value: False
training_configs:
  epochs: 100
  eval_steps: 1
  checkpoint_steps: 1
  device: 0
  random_seed: 1234
  outputs_dir: "train_outputs"
  training_type: "pt"
  wandb_project: "doggy_ai_pt"
  wandb_name: "doggy_ai_pt_large_full"
validation_configs:
  substitution_matrix: "PAM30"
  gap_insertion_penalty: -11
  gap_extension_penalty: -1
  sample_method: "topk"
  beam_width: 1
  top_k: 1
